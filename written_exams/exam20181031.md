### <p align="center"> _Master in High Performance Computing_ </p>

## <p align="center"> "Foundations of HPC" course </p>

# <p align="center">Exam Questions </p>

<p align="center"> Wed 31 Oct 2018 </p>

 ###  Q1: Which value has the speedup of a parallel program that achieves an efficiency of 75% on 32 processors?

 
- [ ]   18
- [x]  24
- [ ]  20
- [ ] 16

Explanation: S = E X T_p

### Q2: Your application is 60% of time doing I/O. The vendor is proposing you to change the CPU with one that is 10X faster. Using Ahmdal law estimate the speed-up you can get and choose the answer accordingly:

- [ ] The bandwidth between CPU and RAM is the problem! Does the new CPU increase it?
- [ ]  Great deal: please bring two cpu instead of one so i can get even more performance
- [ ] I dunno.. I have to think a little bit more
- [x] Dude, did you understand what is my problem ?
 
Explanation: There is no link between CPU and I/O, dude!
  
### Q3 In what regard the so-called "Moore's law" is involved in the deployment of new commodity microprocessors?
- [ ] it is basically a physical law that governs the 2-folding of computational power in a time interval $T$;
- [ ] it states the effort of evolving the microprocessors technology, establishing an industrial target;
- [x] it is a forecast of technological capability, grounded on physical and technological motivations, that holds since about 40 years.

Explanation: It is not a physical law, it is a forecast.

### Q3b It is usually reported as a given quantity doubling every $N$ months. Which that quantity is:

* [ ] Computer performance;
* [ ] CPU clock speed;
* [x] the number of transistor per chip;
* [x] one of the above, at a constant manufacturing cost.

Explanation: Those are both right

### Q3c: At the same time, the "Dennard's scaling" casts a relationship among Voltage, Current and Frequency of transistors. Which factors, one or more among the following, determined the so-called "power wall" about 15 yrs ago?

* [ ] the leakage current;
* [ ] a physical bottom limit to the length of transistors;
* [ ] a physical bottom limit to the voltage;
* [x] a physical limit in the possible frequency at which transistors could be switched on and off;
* [ ] none of the above.


### Q4: Why is Cache Important?

- [ ] It acts as a way for parallel programs to access another processor's memory.
- [ ] Cache can tunnel data to the processor efficiently keeping it busy.
- [x] Cache is very fast memory
- [ ] It speeds up disk access.

Explanation: It's actually so simple.

### Q5: What is Cache Thrashing?

- [ ]  It is the heavy use of cache to improve performance.
- [ ]  It is when data does not reside in cache and has to be fetched from main memory.
- [x]  It is when successive data reads overwrite the previous read's cache.
- [ ] It is when you change the loop structure of your program to optimize cache use.

Explanation: It is trivial.

### Q6: You are profiling a code to optimize it. At first glance, you have both a high branch-miss and a high L1D+L1I cache miss fractions. What will be most likely the first thing to consider to optimize that code ?

* [x] its workflow;
* [ ] the memory reference / data layout
* [ ] none of the above

Explanation: High branch miss is a consequence. The fact that L1I is included means it is workflow since data doesn't matter in this case. If it was only L1D, could have been both answers

### Q8:  The following loop exhibits a loop-carried dependency that hampers the loop parallelization. 
```C
for ( i = 0; i < N; i++ )
	{
		a[ i ] = a[ i ] + b[ i ];		// statement S1
		b[ i+1 ] = c[ i ] + d[ i ];		// statement S2
	}
```
The dependency is:

* [ ] on `a[i]` ;
* [x] on `b[i]`.
* [ ] on `c[i]`

Explanation: The first one can be written as a[i] += b[i], so the dependency is on b[i].

### Q8b: That dependency could be removed. Check hat it seems to you to be correct here below:

* [ ]  case 1
```C
  a[ 0 ] = a[ 0 ] + b[ 0 ];
  
  for ( i = 0; i < N-2; i++ )
  	{
      	b[ i+1 ] = c[ i ] + d[ i ];
      	a[ i+1 ] = a[ i+1 ] + b[ i+1 ];
  	}
  b[ N-1 ] = c[ N-2 ] + d[ N-2 ];
  ```

* [ ]  case 2:
```C
  a[ 0 ] = a[ 0 ] + b[ 0 ];
  
  for ( i = 1; i < N; i++ )
  	{
      	b[ i ] = c[ i-1 ] + d[ i-1 ];
      	a[ i ] = a[ i ] + b[ i ];
  	}
  ```

* [x] none of the above codes.

Explanation: No result takes in account that b[N] is set in the original loop since b[i + 1] with i < N.

### Q9 Say that for some reasons you have to solve the following problem:

you have a cubic grid $$N \times N \times N$$, that you represent in memory as an array $C[i][j][k]$, where $i$,$j$,$k$ are in row major order (then $i$ being the slowest changing position in memory).
You need to assign to each point its distance from the origin $C[0][0][0]$, and you have inherited this obvious code snippet:

```c
for ( i = 0; i < N; i++ )
    for ( j = 0; j < N; j++ )
        for ( k = 0; k < N; k++ )
            C[i][j][k] = sqrt(i*i + j*j + k*k);
```

There are some obvious small things that must be changed for this code to be more efficient, and we could talk about that at the oral examination.
However, there may be other issues to be considered. For instance, there certainly is a symmetry in this problem, that could be exploited in the following way:

```c
for ( i = 0; i < N; i++ )
    for ( j = 0; j <= i; j++ )
        for ( k = 0; k < N; k++ )
        	{
            	double dist = sqrt(i*i + j*j + k*k); 
            	C[i][j][k] = dist;
            	if ( i - j )
            		C[j][i][k] = dist;
        	}
            	
```

Is this code correct?

* [ ] yes
* [ ] no

Is this code more efficient than the previous one in all cases ?
* [ ] yes
* [ ] no
  
Explanation: To be discussed by Luca Tornatore.
  
### Q10:  How inlining calls to subprograms shorten program execution?

 
- [ ] The loop overhead in the subprograms are eliminated because the loop is replaced by the collection of lines it would have executed
- [x] The subprogram call overhead is completely eliminated
- [ ] The subprogram body is converted into a single line of code which is executed as is
- [ ] Inlining reduces the size of your executable and so its runs faster

Explanation: The application doesn't have to search for function content in other files.

### Q11:  What is the difference between a pipelined and non-pipelined functional unit?


- [ ]  There is no difference, both functional units types calculate some operation on input data, in the same way.
- [ ]  A non-pipelined functional unit will perform a series of calculations faster. The functional unit is not split up into FDELW order.
- [x]  A pipelined functional unit will perform a series of calculations faster. The functional unit is split up into correct FDELW order.
- [ ]  Any functional unit can be pipelined, no matter what calculation it performs. Only certain function units can be non-pipelined.

Explanation: FDELW = Fetch, Decode, Execute, Load, Write. 

###  Q12: Which of the following libraries does not implement BLAS routines ?
  
- [ ] GOTO
- [ ] MKL
- [x] FFTW
- [ ] ATLAS

Explanation: HPL is compiled against a LA library which is MKL. ATLAS is Automatically-Tuned LA System, so it still uses BLAS. FFTW is Fast-Fourier Transform, which is not LA operations. GOTO is a very efficient matrix-matrix multiplication library. 

### Q13: Blocking is a technique used in scientific SW development to enhance:
- [ ] compiler optimization
- [x] data access into cache memory
- [ ] threads access to shared memory
- [ ] none of the above

Explication: Trivial.