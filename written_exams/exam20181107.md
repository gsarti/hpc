### <p align="center"> _Master in High Performance Computing_ </p>

## <p align="center"> "Foundations of HPC" course </p>

# <p align="center">Exam Questions </p>

<p align="center"> Wed 07 Nov 2018 </p>

 ###  Q1 *[2 points]*: What is the overall speedup if you make a section of your program that takes 10% of  time 90 times faster?  

- [x]  around 1.1  
- [ ] greater than 5 
- [ ] exactly 2
- [ ] no speedup at all 

Explanation: The overall speedup of a small fraction of the code is small.

### Q2 *[2 points]*: Why HPL is no longer indicated to measure the overall performance of the HPC system ?

-  [ ] HPL computational workload is no longer able to stress the large HPC infrastructure 
-  [x]  HPL is only able to stress the floating point unit and not the overall infrastructure
-  [ ] HPL is too old to be representative of modern architecture
-  [ ] HPL is memory bounded and memory is too slow on modern CPU

Explanation: HPCG is the solution to that.

### Q3 *[2 points]*: Indicate which of the following is a realistic capacity unit for a L1 cache memory:

-   [x] KByte
-   [ ] MByte
-   [ ] Byte
-   [ ] none of the above

### Q4 *[2 points]*: I submit a job to compile it and run my openMP application by means of the following script on Ulysses

```
#PBS -l walltime=3:00:00  
#PBS -l nodes=1:ppn=20 

module load gnu

gcc my_openmp.c -o my_openmp.x

./my_openmp.x

exit 
```
How many threads does it spawn once executed ?

- [ ]  20 threads 
- [x] 1 thread
- [ ]  the compilation is wrong
- [ ] no way to know this at this stage

Explanation: We are not linking -fopenmp, so the application is serial running on a single thread.

### Q5 *[2 points]*: on a NUMA multicore, multisocket architecture which of the following statements is wrong ?

-   [ ] caches are NOT shared among different sockets
-   [ ] I can access all the memory locations from any of the cores of my system 
-   [x] Time to access any location of the memory is independent from the core I am using
-  [ ]  L1 cache is usually private to the core

Explanation: By definition, Non-Uniform Memory Access architecture performance depends on location of memory.

### Q6 *[2 points]*: Indicate which of the following configuration can fit a 256bit register:

-  [ ] 8 doubles
-  [x] 4 doubles
-  [ ] 4 floats
-   [ ] none of the above

Explanation: 256bit -> 32 bytes. A double takes 8 bytes.

### Q7 *[2 points]*: Indicate which of the following sentences is appropriate to vectorization :  

-   [ ] different operations on multiple data  
-   [ ] a += b[i] * c[i] is likely to be vectorized by the compiler  
-   [x] a[i] = b[i] * c[i] is likely to be vectorized by the compiler  
-   [ ]  software development has no impact on vectorization

Explanation: Multiplication is easily vectorizable.

### Q8 *[2 points]*: A memory bound problem is usually mostly limited by:

-  [x] memory bandwidth
-  [ ] memory capacity
-  [ ] shared memory
-  [ ] none of the above

Explanation: Memory capacity could've been right, but right now memory bandwidth is much more relevant and fit the memory bound definition better.

### Q9 *[2 points]*: You have to write two proposals to ask for  two large time slots on a HPC facility. Your aim is twofold. On proposal 1 , you want to enlarge the size of a certain problem that you are investigating. On proposal 2, you have a large space parameters to cover, and hence you need to run a large number of not-so-big simulations. The call requires that you already have a production-level code, and that you demonstrate that this code is adequate to efficiently use the cores that will be dedicated to your project. How would you prove that in each of the two proposals?  Choose one or more among the following options:
* [ ] you provide a detailed analysis of your code using the performance counters, namely: both L1D+L1I and L2D cache misses, branche misses and Instructions-per-Cycles;
* [ ] you provide access to some repository that hosts the code;
* [ ] you provide a strong-scaling test and a weak-scaling test for respectively the former and the latter proposals;
* [x] you provide a weak-scaling test and a strong-scaling test for respectively the former and the latter proposals;

Explanation: Should be weak scalability for both, since we are basically doing the same test more times, on bigger sizes.


### Q10 *[2 points]*: What is the key differences between a process and a thread?
* [ ] basically they are the same thing; the difference in naming is related to the way you launch them;
* [ ] a thread is a task performed by the CPU, whereas a process is a program in the user space;
* [ ] a process is a segment of code with its own allocated memory, whereas a thread is basically a function within a program;
* [x] a process is a executing instance of a program, with its own entry point, PID and virtual address space and is launched of one thread; a thread is the basic unit to which the O.S. allocates processor time: it is an entity within a process that can executes concurrently with other threads spawned by the process;

Explanation: Seems accurate.

### Q11 *[2 points]*: What are the key differences between the stack and the heap in the *niX execution model? Choose one or more among the following options:
* [ ] none: they are two equivalent memory regions, the programmer can choose which one to use;
* [ ] the stack is used just to keep track of where (i.e. in which function) the running process is in each moment, while the heap contains all the data;
* [x] the heap is used for dynamic allocation of memory;
* [ ] the stack can not be used for dynamic allocation;
* [x] the stack contains the local variables of the functions, its arguments, its returning value and the details needed to continue the execution after the end of a function; 

### Q12 *[3 points]*: a serial code takes 90 minutes to execute 10000 iteration. Each iteration can be solved in parallel using multiple threads, but the overhead to start the threads is 100 millisecond. Taking into account that the initialization phase is serial and takes 10 minutes regardless the number of iterations then executed, how many core we need to bring the overall execution time to 12 minutes for 1000 iteration ? 

-  [ ] >4
-  [x] >=32
-  [ ] >=64
-  [ ] exactly 45

Explanation: Basic computation.

### Q13 *[3 points]*: Spot what's wrong in the following code:

```C
double * allocate_and_copy_array( int N, double **copy_array )
{
	double *array, *array_copy;
	int i;
	
	array = (double*) calloc( N, sizeof(double) );
    double N_inv = 1.0 / N;
	for ( i = 0; i < N; i++ )
		array[ i ] = sin(i * N_inv);
	
    array_copy = (double*) calloc(N, sizeof(double));
    memcpy(array_copy, array, N*sizeof(double));
    
    copy_array = &array_copy;
	return &array[0];
}
```
* [ ] the returned value;
* [ ] the returned argument;
* [ ] none of the above, write your opinion:

<br><br>

### Q14 *[3 points]*:  You need to optimize the following piece of code. 

```C
#include <stdlib.h>
#include <stdio.h>
#define SIZE 1024

int main (int arcg, char * argv[]){

  int i;
  double k = 1.232;
  double w = 5.456;
  double e_tot, y;

  fscanf( stdout, "Please insert the value of y: %.3g", y );

  for( i = 0; i < SIZE; i++){
    e_tot += ( ( k * w ) / y ) / i;
  }

  return 0;
}
```

Which of the following is surely wrong:

- [ ]  compile with -O3 
- [ ]  compute ( k*w/y ) outside the loop and use just the result within the loop
- [ ] compute the inverse of i and use it within the loop 
- [ ]  unroll the loop  

Explanation: -O3 is a good idea, the second is also a good idea. Unrolling can also work, while the third one doesn't make any sense.

### Q15 *[3 points]*: Which is an optimal program code from the viewpoint of cache performance tuning ? 

- [ ] code a  
```C
for (  int itr=0; itr<=200; itr++) {
	 for ( int i=1; i <NX; i++) { 
		  for ( int j=1; j <NY; j++) { 
               a[i][j]=b[i][j]+c[j][i];
            }
      }
}
```
- [ ] code b

```C
for (  int itr=0; itr<=200; itr++) {
	 for ( int j=1; j <NX; j++) { 
		  for ( int i=1; i <NY; i++) { 
               a[i][j]=b[i][j]+c[j][i];
            }
      }
}
```

- [ ] code c

```C
for ( int j=1; j <NY; j++) { 
   for ( int i=1; i <NX; i++) { 
               d[i][j]=c[j][i];
            }
      }

for (  int itr=0; itr<=200; itr++) {
	 for ( int j=1; j <NX; j++) { 
		  for ( int i=1; i <NY; i++) { 
               a[i][j]=b[i][j]+d[i][j];
            }
      }
}
```

- [x] code d

```C
for ( int i=1; i <NX; i++) { 
   for ( int j=1; j <NY; j++) { 
               d[i][j]=c[j][i];
            }
      }

for ( int itr=0; itr<=200; itr++) {
	 for ( int i=1; i <NX; i++) { 
		  for ( int j=1; j <NY; j++) { 
               a[i][j]=b[i][j]+d[i][j];
            }
      }
}
```

Explanation: We are looping in an optimal C way in d, but we transpose c into d in order to have optimal access for the 200 cycles.

### Q16 *[3 points]*: You have a CPU-memory system with 2 levels of cache, with the following access penalties in units of CPU cycles:

| Level | penalty |
| :---: | :-----: |
|  L1   |  2 cyc  |
|  L2   |  4cyc   |
| DRAM  | 100 cyc |

How much does it cost the access to memory in CPU cycles if you have a  L1D miss-rate  of  10 %, and the L2 miss-rate of 5%  ? 

- [x] 2.9
- [ ] 1.05
- [ ] 60.45
- [ ] it does NOT depend on L1D and L2 miss-rate.

Explanation: 2 is brought by the L1 miss rate, 0.4 is brought by the L2 miss rate and 0.5 is the combined L1 + L2 miss rate.